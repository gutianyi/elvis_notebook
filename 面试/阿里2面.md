[TOC]

#### 把线性回归  lstm  xgboost 这三者分类  你个人的想法

#### 线性回归处理问题有什么优势  为什么在你的项目里面表现的比较好

#### 为什么项目里面用前14天预测  和  每天数据作为input size 有什么区别

#### L1 L2正则化的区别和应用场景

L1正则化有助于生成一个稀疏权值矩阵

#### XGBOOST 介绍  与GDBT区别    有线性模型吗   有什么提升 在工程上的理解

优点：

1. 正则化

2. 并行处理

3. 灵活性

   > 自定义损失函数

4. 内置交叉验证 .cv()

5. 缺失值处理

6.  剪枝

   > Xgboost先从顶到底建立所有可以建立的子树，再从底到顶反向机芯剪枝，比起GBM，这样不容易陷入局部最优解

#### 归一化的好处 在梯度下降中的表现   用数学解释

#### LSTM 介绍  解决梯度爆炸原理解释

#### DNN 中如何解决神经网络中梯度消失，爆炸问题?

1. 换激活函数 使用relu (梯度消失)

2. 初始化权重

3. 权重正则化

4. 梯度裁剪

   常见的 gradient clipping 有两种做法

   * 根据梯度的值直接进行裁剪

   * 根据若干参数的梯度组成的向量的 L2 正则化进行裁剪 

     > 第二种方法则更为常见，先设定一个clip_norm, 然后在某一次反向传播后，通过各个参数的 gradient 构成一个 vector，计算这个 vector 的 L2 norm（平方和后开根号）记为LNorm，然后比较LNorm和clip_norm的值，若LNorm<=clip_norm不做处理，否则计算缩放因子scale_factor=clip_norm/LNorm，然后令原来的梯度乘上这个缩放因子。这样做是为了让 gradient vector 的 L2 norm 小于预设的clip_norm。
     > 对参考的见解

4. BN 消除了x带来的放大缩小的影响
5. 残差结构