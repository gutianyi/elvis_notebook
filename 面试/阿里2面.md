[TOC]

#### 把线性回归  lstm  xgboost 这三者分类  你个人的想法

#### 线性回归处理问题有什么优势  为什么在你的项目里面表现的比较好

#### 为什么项目里面用前14天预测  和  每天数据作为input size 有什么区别

#### L1 L2正则化的区别和应用场景

L1正则化有助于生成一个稀疏权值矩阵 

> 如果模型中的特征之间有相互关系，会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时我们就要进行特征选择。特征选择也会让模型变得容易解释，假设我们要分析究竟是哪些原因触发了事件A，但是现在有1000个可能的影响因子，无从分析。如果通过训练让一部分因子为0，只剩下了几个因子，那么这几个因子就是触发事件A的关键原因。进行特征自动选择，也就是让模型变得稀疏，这可以通过L1范数实现

> 二维空间下来看 解空间是一个正方形  相交的点很容易在 别的维度参数是0的时候

L2正则化通过权重衰减，保证了模型的简单，提高了泛化能力

#### XGBOOST 介绍  与GDBT区别    有线性模型吗   有什么提升 在工程上的理解

优点：

1. 正则化

2. 并行处理

3. 灵活性

   > 自定义损失函数

4. 内置交叉验证 .cv()

5. 缺失值处理

6.  剪枝

   > Xgboost先从顶到底建立所有可以建立的子树，再从底到顶反向机芯剪枝，比起GBM，这样不容易陷入局部最优解

#### 归一化的好处 在梯度下降中的表现   用数学解释

#### LSTM 介绍  解决梯度爆炸原理解释

#### DNN 中如何解决神经网络中梯度消失，爆炸问题?

1. 换激活函数 使用relu (梯度消失)

2. 初始化权重

3. 权重正则化

4. BN 消除了x带来的放大缩小的影响

5. 残差结构

6. 梯度裁剪

   常见的 gradient clipping 有两种做法

   * 根据梯度的值直接进行裁剪

   * 根据若干参数的梯度组成的向量的 L2 正则化进行裁剪 

     > 第二种方法则更为常见，先设定一个clip_norm, 然后在某一次反向传播后，通过各个参数的 gradient 构成一个 vector，计算这个 vector 的 L2 norm（平方和后开根号）记为LNorm，然后比较LNorm和clip_norm的值，若LNorm<=clip_norm不做处理，否则计算缩放因子scale_factor=clip_norm/LNorm，然后令原来的梯度乘上这个缩放因子。这样做是为了让 gradient vector 的 L2 norm 小于预设的clip_norm。
     > 对参考的见解