[TOC]



### ML

#### 机器学习经典算法优缺点总结

##### 线性回归

优点：

- 可解释性强
- 实现简单  计算量小  存储资源低
- 能通过正则化来避免过拟合
- 容易使用sgd更新模型

缺点：

- 无法处理非线性关系
- 特征空间很大时候 性能不好
- 容易欠拟合  准确度一般不高

##### 树

优点：

- 能学习非线性关系
- 可解释性强  白盒
- 对异常值也具有很强的稳健性
- 不需要数据预处理 ：*null or 标准化*
- 能够同时处理类别和数据型属性
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果

缺点：

- 单棵树容易过拟合
- 忽略数据集中属性之间的相关性

##### NN

优点：

- 能适用于多种问题
- 隐藏层还能降低算法对特征工程的依赖

缺点：

- 黑盒
- 需要大量的数据
- 调参麻烦

##### KNN

优点：

- 简单有效

缺点：

- lazy learning  不学习
- K值选择没有理论去选择最优
- 容易受样本不平衡数据集影响
- 每次分类都会重新进行一个全局运算
- 内存密集型算法  处理高维数据时的效果并不理想，同时还需要高效的距离函数来计算相似度

##### 逻辑回归

优点：

- 输出结果会有很好的概率解释
- 能通过正则化以避免过拟合
- 容易使用sgd更新模型

缺点：

- 面对多元或非线性决策边界时性能较差
- 遭受多重共线性

##### 朴素贝叶斯

优点：

- 没有分布要求
- 比逻辑回归等区分性模型更快地收敛，因此在小规模的数据上表现很好 
- 适合增量式训练
- 对缺失数据不太敏感，算法也比较简单

缺点：

- 需要条件独立假设实际成立  遭受多重共线性
- 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的
- 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率

##### SVM

优点：

- 可以解决小样本情况下的机器学习问题
- 可以提高泛化性能
- 可以解决高维问题
- 可以解决非线性问题 有许多可选的核函数

缺点：

- 选择正确的核函数需要技巧
- 是内存密集型算法 不太适用较大的数据集
- 对缺失数据敏感
- 线性svm需要归一化(依赖数据表达的距离测度)  lr不需要

##### KMeans

优点：

- 足够快速、足够简单

缺点：

- 鲁棒性差  容易受到噪音干扰
- K值选择问题
- 对数据分布有要求

#### 逻辑回归的原理

线性回归是去求出一条拟合特征空间中所有点的线，逻辑回归本质是相同的 但是加了一个sigmoid的外壳  用sigmoid来转换线性回归的输出来返回概率值， 然后根据概率值映射到两个离散类。

#### 为何逻辑回归用sigmoid

逻辑回归对应logit function，也就是说逻辑回归就是sigmoid的应用（一个对象的两个描述而已）

为什么会有这个定义呢？ 

> lr是基于伯努利分布为假设的，伯努利分布的指数族分布形式就是sigmoid函数，而且sigmoid函数可以将数据压缩到0-1内，以便表示概率

> 1. sigmoid 函数连续，单调递增
> 2. p′=p∗(1−p)计算sigmoid函数的导数非常的快速

#### 为何逻辑回归用交叉熵函数

> mse的导数里面有sigmoid函数的导数，而交叉熵导数里面没有sigmoid函数的导数，sigmoid的导数的最大值为0.25，更新数据时太慢了

不用mse的原因是：

1. MSE会有梯度消失现象
2. MSE的导数非凸函数，求解最优解困难

#### 为啥离散特征用one-hot

#### 类别不平衡

#### xgb流程

http://datacruiser.io/2019/08/10/DataWhale-Workout-No-8-XGboost-Summary/

![](https://raw.githubusercontent.com/gutianyi/image/master/img/20201004150527.png)

- **计算分裂前后损失函数的差值 即score(gini指数)**

#### xgboost和gbdt的区别

首先呢，xgboost是gbdt的一种高效系统实现。

- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，

  > （这个时候xgboost相当于带L1和L2正则化项的逻辑回归（分类问题）或者线性回归（回归问题）。）

- GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，

  > （同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。）

- xgboost在代价函数里加入了正则项，

  >  (用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和  从而来防止过拟合)

- 对缺失值的处理。

- xgboost工具支持并行。data事先排好序并以block的形式存储，利于并行计算

  > xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

- 列抽样
- 支持分布式

#### 怎么判断过拟合，你一般怎么解决

模型在验证集合上和训练集合上表现都很好，而在测试集合上变现很差。

解决：

- 增加正则项
- 获取更多的训练数据
- 降低模型复杂度（减少网络层数
- 用集成学习的方法 stacking
- 对于NN做 dropout|  BN | early stop
- 标签平滑
- 数据增强





2.lr和xgboost有什么区别 

 3.什么情况下用lr比较好 

 4.lr和svm的区别 

 5.bagging和boosting算法介绍一下 

 6.如何解决过拟合和欠拟合的问题 

 7.数据增强有什么方法 

 8.xgboost是如何训练的 

 9.kmeans的具体步骤 

 10.如何评价聚类结果的好坏。轮廓系数有没有用过。 

 11.roc曲线介绍一下。

#### 参数初始化的作用是啥

如果全部初始化为0，在神经网络第一遍前向传播所有隐层神经网络激活值相同，反向传播权重更新也相同，导致隐层神经元没有区分性，称为“对称权重”现象。为打破这个平衡，比较好的方式是对每个参数进行随机初始化。

> 每一层参数更新后的值都是相同的。后面不断的正向和反向传播，每一层的参数更新之后的值都是相同的。那么带来的问题就是：**所有参数值一样，意味着不同的结点根本无法学习到不同的特征，通过不同结点的输出值始终是相同的。这就失去了神经网络特征学习的意义。换句话说，每层所有结点的值都一样，就相当于该层只有一个结点发挥了作用。因此初始化全为0很有可能导致模型失败，无法收敛。这种现象称为“对称权重现象”。**

#### 偏差和方差

偏差(bias): 预测值与真实值之间的距离  偏差越大，越偏离真实数据

方差(variance)：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。 方差越大，数据的分布越分散

#### 什么模型能减小方差

Bagging   可以假设不同的训练集为x_1 x_2  两者完全独立：
$$
\begin{aligned}
\operatorname{Var}\left(\frac{\sum X_{i}}{n}\right) &=\operatorname{Var}\left(\frac{X_{1}+X_{2}}{2}\right)=\frac{1}{4} \operatorname{Var}\left(X_{1}+X_{2}\right)=\frac{1}{4}\left(\operatorname{Var}\left(X_{1}\right)+\operatorname{Var}\left(X_{2}\right)\right)=\\
\frac{1}{4}\left(2 \operatorname{Var}\left(X_{1}\right)\right)=& \frac{\operatorname{Var}\left(X_{i}\right)}{2}=\frac{\operatorname{Var}\left(X_{i}\right)}{n}
\end{aligned}
$$

#### xgboost的二阶泰勒展开为啥效果那么好？

Xgboost使用二阶展开效果更好的原因，应该与牛顿法使用海塞矩阵比SGD好的原因一样 

> 二阶信息本身就能让梯度收敛更快更准确 ;可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。这是从二阶导本身的性质，也就是**为什么要用泰勒二阶展开**的角度来说的

#### XGB LGB Catboost 分裂区别

这里的分裂过程实际上是这样的，我们以上图为例：

首先Height>170分裂，

1、则左节点A为height>170,右节点B为heigh<170；

2、此时我们有两个叶子节点A和B，正常按照xgb的做法，A继续在所有feature里research找分裂增益最大的叶子节点，B也是在所有feature里面找分裂增益最大的，比如A找到weight>65为分裂节点继续分裂，而B找到age>20为分裂节点继续分裂，这是传统的level-wise；而按照lgb的做法，A找到某个最优特征的最优分裂点，假设其增益gain为x1，而B找到的某个最优特征的最佳分裂点，假设其增益gain为x2，此时进行比较，如果x1>x2，则分裂A不分裂B，如果x 1<x2则分裂B而不分裂A，此为leaf-wise； **而catboost的做法和lgb有点类似，也是进行x1和x2的比较，但是比较完毕之后，是对A和B两个节点都使用B所得到的最优特征的最佳节点进行分裂**。



#### B+🌲

#### 降维算法  PCA怎么做的

#### 逻辑回归和线性回归的区别

#### 激活函数优缺点

#### 优化器的优缺点（SGD MOM* RMSPROPS ADAM）

#### 为何逻辑回归用sigmoid，为何逻辑回归用交叉熵函数，为啥离散特征用one-hot

#### L1 L2正则

> 从统计概率上讲，L1是laplace分布，L2是高斯分布，L1的分布对极端值能更加容忍也就是说L2比L1对outlier 大数更加敏感，对于L1更可以防止过拟合

L2 相比于 L1 对于异常值更敏感 对于L1更可以防止过拟合(因为平方的原因, L2 对于大数的乘法比对小数的惩罚大)

从求导上看 L2是连续的  计算方便 有唯一解, 约靠近0的时候 梯度就小 越无法接近0；L1不是连续的 但是梯度是固定1或者-1  L1 正则的话基本上经过一定步数后很可能变为0

L2使得权重平滑 ；L1使得权重稀疏，会把不重要的特征直接置零（从而起到特征筛选的作用）

L2 是优化常用方法



### NN

#### NLP

##### word2vec和fasttext的区别

#### LSTM

##### 1.LSTM在训练时，每个时刻的weight是否是同时变化？

##### 2.LSTM在训练时，每个时刻的weight是否会相互影响？

##### 3.简述RNN，LSTM，GRU的区别和联系

RNN：

<img src="https://raw.githubusercontent.com/gutianyi/image/master/img/rnn.jpg" style="zoom: 15%;" />

> 把RNN扩展成一个三层全神经网络(input hidden output)，每输入一步都是共享参数U V W

LSTM：



##### 4.画出lstm的结构图，写出公式

##### 5.RNN的梯度消失问题？如何解决？

##### 6.lstm中是否可以用relu作为激活函数？

##### 7.lstm各个门分别使用什么激活函数？

##### 8.LSTM 能解决 RNN 什么问题，并通过数学公式推导

##### 9.双向 LSTM 比 LSTM 好在哪

##### 10.介绍一下 LSTM 的各个门

##### 11.Transformer与LSTM区别

##### 12.Transformer相对RNN为什么能避免梯度消失



##### 梯度爆炸

对激活函数进行求导，如果导数大于1，那么随着网络层数的增加，也就是说参数是一个连乘的结果   梯度更新将会朝着指数爆炸的方式增加。同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。

##### BN 

BN的假设是如果每一层每单个神经元输入分布是归一化的，那么这将有利于优化

BN的作用是去平滑优化空间 使得梯度变得平滑稳定  正则化的作用是副产物

##### Dropout

### 大数据

##### 什么是Map，什么是Reduce?

考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。Map阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似(hello, 12100次)，(world，15214次)等等这样的Pair(我这里把Map和Combine放在一起说以便简化);这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果(当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊)。然后这些Reducer将再次汇总，(hello，12100)+(hello，12311)+(hello，345881)= (hello，370292)。每个Reducer都如上处理，你就得到了整个文件的词频结果。

##### Hadoop:

可扩展的、用于分布式计算的分布式系统基础架构。简单的说Hadoop软件库就是一个框架，通过这个框架你可以在计算机集群中对大规模的数据集进行分布式处理，可以理解成和操作系统的多线程并行有点像的一个东西。但是这个东西非常稳定，如果一个节点上的数据宕掉了还有神奇的机制可以保证数据备份在其他节点上，可以继续运行。

核心是HDFS Mapreduce YARN Hive

HDFS: 分布式文件系统

> GFS的实现  https://zhuanlan.zhihu.com/p/51202200
>
> namenode 管理整个文件系统的命名空间，用于存储元数据及处理客户端发送的请求。
>
>  datenode  文件以数据块的形式进行存储

Mapreduce： 分布式计算模型  

> 它将计算抽象成Map和Reduce两部分，其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果。

YARN：分布式资源管理器    

> 作业调度和资源管理

Hive：

> 将SQL转化为MapReduce任务在Hadoop上运行，通常用于离线分析。

ZooKeeper：分布式协作服务

##### spark和mapreduce区别：

Hadoop和Spark都是大数据计算平台

1. 存储：Hadoop有自己的一套线下存储集群；Spark没有存储集群，所以Spark需要和一个云计算平台相结合，一般都是和Hadoop相结合
2. 计算性能：Hadoop基于MapReduce来运行计算，MapReduce可以简单理解为：一个人数一堆散牌里有多少个红桃比较慢，但是你把这堆牌分给几个人一起去数，这叫Map,然后把这几个人的结果汇总到一起，这叫Reduce。由于MapReduce需要把Map的结果写到存储集群里，然后Reduce再去读集群处理后的结果再去运算，这样来回读取集群存储比较慢；而Spark计算时是把计算结果写到内存里，然后再实时读取内存，所以运行结果快，一般可以比Hadoop快10-100倍，所以Spark多用于流计算等实时处理中。

  > Spark 能够比 Hadoop 运算更快，主要原因是：Hadoop 在一次 MapReduce 运算之后，会将数据的运算结果从内存写入到磁盘中，第二次 MapReduce 运算时在从磁盘中读取数据，两次对磁盘的操作，增加了多余的 IO 消耗；而 Spark 则是将数据一直缓存在内存中，运算时直接从内存读取数据，只有在必要时，才将部分数据写入到磁盘中。除此之外，Spark 使用最先进的 DAG（Directed Acyclic Graph,有向无环图）调度程序、查询优化器和物理执行引擎，在处理批量处理以及处理流数据时具有较高的性能。
4. 灾备恢复：Hadoop有自己的一套离线存储集群，天生具有灾备恢复能力；Spark的数据对象存储分布于数据集群中的叫做弹性分布式数据集(即RDD)，故也有灾备恢复能力。

##### spark

Spark 除了 Spark Core 外，还有其它由多个组件组成，目前主要有四个组件：Spark SQL、Spark Streaming、MLlib、GraphX。

<img src="https://raw.githubusercontent.com/gutianyi/image/master/img/20200928210059.png" style="zoom:67%;" />

**Spark Core：**是 Spark 的核心，主要负责任务调度等管理功能。Spark
Core 的实现依赖于 RDDs（Resilient Distributed Datasets,
弹性分布式数据集）的程序抽象概念。

**Spark SQL：**是 Spark 处理结构化数据的模块，该模块旨在将熟悉的 SQL 数据库查询与更复杂的基于算法的分析相结合，Spark
SQL 支持开源 Hive 项目及其类似 SQL 的 HiveQL 查询语法。Spark
SQL 还支持 JDBC 和 ODBC 连接，能够直接连接现有的数据库。

**Spark Streaming：**这个模块主要是对流数据的处理，支持流数据的可伸缩和容错处理，可以与 Flume（针对数据日志进行优化的一个系统）和 Kafka（针对分布式消息传递进行优化的流处理平台）等已建立的数据源集成。Spark Streaming 的实现，也使用 RDD 抽象的概念，使得在为流数据（如批量历史日志数据）编写应用程序时，能够更灵活，也更容易实现。

**MLlib：**主要用于机器学习领域，它实现了一系列常用的机器学习和统计算法，如分类、回归、聚类、主成分分析等算法。

**GraphX：**这个模块主要支持数据图的分析和计算，并支持图形处理的 Pregel API 版本。GraphX 包含了许多被广泛理解的图形算法，如 PageRank。	