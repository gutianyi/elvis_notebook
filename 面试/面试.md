[toc]



#### GBDT

首先 GBDT 是集成学习的一种，而且属于 boosting 家族。我们知道 Bagging 的思想比较简单，即每一次从原始数据中根据均匀概

率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再

对分类器进行组合。常见的随机森林算法就是 bagging 的集成算法。

boosting 的每一次抽样的样本分布都是不一样的。每一次送代，都根据上一次送代的结果，増加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是 boosting 思想的本质所在。送代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调塾和分类器的集成是我们需要考虑的关键问题。传统的 Adaboost，利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去 GBDT 同样也是一种送代的方法。使用了前向分布算法，但是弱学习器限定了只能使用 CART 回归树模型，同时迭代思路和 Adaboosttb 有所不同

### 题目

#### 统计概率问题

##### 概率题

> 一个地区的某个疾病的患病率为 0.01， 一个模型能够预测患病与否，错误率为 0.01， 现在，我已经被检测出来患病了， 求我真正患病的概率。

假设B为检验阳性   A为患病概率

$P(B)=P(B|A)*P(A)+P(B|A')*P(A')$

$P(A | B)=\frac{P(A B)}{P(B)}=\frac{P(B | A) * P(A)}{P(B)}$

#### 机器学习面试问题

##### 机器学习经典算法优缺点总结

[机器学习经典算法优缺点总结](https://zhuanlan.zhihu.com/p/46831267)

> 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。——周志华《机器学习》

我的理解是mse是最小二乘法的实现

---------



##### 欠拟合和过拟合

> 欠拟合: **模型**没有很好地捕捉到数据特征，不能够很好地**拟合数据**  （模型无法得到较低的训练误差）

**解决方法**:

1. 添加其他特征项
2. 减少正则化参数

> 过拟合: 模型把数据学习的太彻底  模型**泛化能力**太差 （模型的训练误差远小于它在测试数据集上的误差）

**解决方法**:

1. 重新清洗数据(原因是数据不纯)

2. 增大训练数据的量(train size)

3. 采用正则化方法

4. nn里面用dropout

5. nn里面可以用数据增强(mixup 或者加噪音)

6. nn里面可以BN LN

7. nn里面可以early stop

8. stacking

9. 伪标签

10. label smooth

    > 将标签强制one-hot的方式使网络过于自信会导致过拟合，因此软化这种编码方式。

--------

##### PCA LDA:

PCA中协方差需要数值去中心化处理

**PCA过程:**

1. 获得协方差矩阵 $C=\frac{1}{m} X X^{T}=\left(\begin{array}{cc}\frac{1}{m} \sum_{i=1}^{m} a_{i}^{2} & \frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i} \\ \frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i} & \frac{1}{m} \sum_{i=1}^{m} b_{i}^{2}\end{array}\right)=\left(\begin{array}{cc}\operatorname{Cov}(a, a) & \operatorname{Cov}(a, b) \\ \operatorname{Cov}(b, a) & \operatorname{Cov}(b, b)\end{array}\right)$
2. 我们要让方差最大  协方差最小 也就是让PCA后的协方差矩阵D接近对角矩阵
3. 求矩阵P使得

$$
\begin{aligned}
D &=\frac{1}{m} Y Y^{T}=\frac{1}{m}(P X)(P X)^{T} \\
=& \frac{1}{m} P X X^{T} P^{T}=P\left(\frac{1}{m} X X^{T}\right) P^{T}=P C P^{T}
\end{aligned}
$$

**主成分分析PCA与线性回归的区别**：

- 线性回归是找`x`与`y`的关系，然后用于预测`y`
- `PCA`是找一个投影面，最小化data到这个投影面的投影误差

**PCA和LDA的相同点**：

1. PCA和LDA都是经典的降维算法；
2. PCA和LDA都假设数据是符合高斯分布的；
3. PCA和LDA都利用了矩阵特征分解的思想。

**PCA和LDA的不同点**

1. PCA是无监督（训练样本无标签）的，LDA是有监督（训练样本有标签）的；
2. PCA是去掉原始数据冗余的维度，LDA是选择一个最佳的投影方向，使得投影后相同类别的数据分布紧凑，不同类别的数据尽量相互远离。
3. LDA最多可以降到k-1维（k是训练样本的类别数量，k-1是因为最后一维的均值可以由前面的k-1维的均值表示）；
4. LDA可能会过拟合数据。

-----

##### 为什么正则化可以解决过拟合

正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。

**L2范数：**

对于下面的线性回归中
$$
\ell\left(w_{1}, w_{2}, b\right)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right)^{2}
$$
将权重参数用向量 𝑤=[𝑤1,𝑤2] 表示，带有 𝐿2 范数惩罚项的新损失函数为
$$
\ell\left(w_{1}, w_{2}, b\right)+\frac{\lambda}{2 n}|w|^{2}
$$

----



##### ml中的损失函数、期望风险、经验风险、结构风险

* 损失函数(cost func): 模型预测的值与真实值之间的差距
* 结构风险 (Structural risk): 可以看做对经验风险的一个优化;是对 经验风险 和期望风险的折中，在经验风险函数后面加一个正则化 项（惩罚项）, 得到结构分险。

> 期望风险与经验风险的区别：
> 期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。
>
> 经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。
>
> 缺点：
> 只考虑经验风险的话，会出现过度拟合现象，即模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办？这就需要结构风险。
>
> 链接：https://www.jianshu.com/p/073a00d69acf

--------



##### 在进行线性回归时，为什么最小二乘法是最优方法？

为什么用欧式距离作为误差度量 （即MSE）

它简单。便于计算。通常所推导得到的问题是凸问题，具有对称性，可导性。通常具有解析解，此外便于通过迭代的方式求解。

缺点: 

误差信号和原信号无关。只要误差信号不变，无论原信号如何，MSE均不变。例如，对于固定误差[1 1 1]，无论加在[1 2 3]产生[2 3 4]还是加在[0 0 0]产生[1 1 1]，MSE的计算结果不变。

无法比较空间 时间上的差别

------



##### lr手推 原理、优缺点以及应用场景(LR是什么假设，损失函数是怎么回事，怎样更新参数什么的)

> lr和线性回归的关系:
>
>  线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

损失函数：交叉熵(cross entropy)
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
$$

--------

##### LR

- 我们在学习一个算法或者模型的时候，一定要学着类比和关联，比如很多人说自己熟悉逻辑回归LR，那么问问自己，下面这些内容你都了解吗？

- - LR归一化问题，什么情况可以不归一化，什么情况必须归一化，为什么
  - 提到LR损失函数要能知道交叉熵，为什么是它，以它为损失函数在优化的是一个什么东西，知道它和KL散度以及相对熵的关系
  - 提到LR的求解方法，比如SGD，知道SGD和BGD的区别，知道不同的GD方法有什么区别和联系，二阶优化算法知道什么，对比offline learning和online learning的区别
  - 提到调参，知道模型不同超参数的含义，以及给定一个特定情况，大概要调整哪些参数，怎么调整
  - 提到LR的正则，知道l1l2的原理，几何解释和概率解释
  - LR的分布式实现逻辑是怎么样的，数据并行和模型并行的区别，P-S架构大概是怎么一回事
  - LR作为一个线性模型，如何拟合非线性情况？特征侧比如离散化，交叉组合，模型比如引入kernel，又可以推广到FM等model上，

- 个人感觉如果一场面试中大部分时间是你在向面试官输出你的知识，而不是等他来问这样的一问一答的话，那么基本就是ok的了

- 关于编程题目，leetcode 多刷刷，一般medium居多

- 关于概率统计和智力逻辑题，这个网上也可以网上搜搜看看~

##### SVM原理

* 作为一种二分类模型，用于在特征空间中寻找间隔最大化的分离超平面。有三种情况，线性可分数据，线性基本可分，线性不可分数据是分别对应硬间隔最大化；引入松弛变量，实现软间隔最大化；引入核函数等方法

* 从SVM基本型 通过拉格朗日乘子法得到对偶问题  而因为具有不等式约束  所以使用KKT条件方法 ----》然后用SMO求解α

https://blog.csdn.net/Mr_KkTian/article/details/53750424

1. 线性可分支持向量机(硬间隔支持向量机)
2. 线性支持向量机(软间隔支持向量机)
3. 非线性支持向量机(核函数+软间隔)

<img src="/Users/elvis/ITProjects/GitHub/elvis_notebook/面试/截屏2020-04-10下午4.45.57.png" alt="截屏2020-04-10下午4.45.57" style="zoom:65%;" />

KKT

<img src="/Users/elvis/ITProjects/GitHub/elvis_notebook/面试/截屏2020-04-11下午4.01.25.png" alt="截屏2020-04-11下午4.01.25" style="zoom:67%;" />

> 核函数：
>
> SVM函数经过拉格朗日乘子法之后得到的对偶问题 把其中$$X_i X_j$$两个向量的内积改成核函数的方法K
>
> 避免了在高维特种空间下内积的计算
>
> **核函数和映射没有关系。**核函数只是用来计算映射到高维空间之后的内积的一种简便方法
>
> 1. 多项式核、高斯核的SVM确实是可以解决线性不可分问题的
> 2. 一般来说，C越大，训练得到的模型越准确
> 3. 如果采用高斯核，参数γ的值对精度影响也非常大

------

##### 最常用的5个回归损失函数

>  (from https://www.jiqizhixin.com/articles/2018-06-21-3)

* 均方误差(L2损失)
  $$
  M S E=\sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}
  $$

* 平均绝对值误差（也称L1损失）

  > MAE对异常点有更好的鲁棒性  
  >
  > 一些情况下L2损失会因为存在一个异常点，而导致误差非常大。

  $$
  M A E=\sum_{i=1}^{n}\left|y_{i}-y_{i}^{p}\right|
  $$

* Log-Cosh损失

  > Log-cosh是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。

-------------



##### LSTM为什么可以解决梯度弥散的问题

https://www.zhihu.com/question/268956632

RNN是先计算结果再去* W，这样累积导致了梯度爆炸和梯度消失，

而LSTM是先把遗忘门的输入✖️w，然后再和输入门进行累加来作为下一刻的memory。

> LSTM可以控制 遗忘门输入的bias来控制遗忘门一直接近1来解决梯度消失

***不管是梯度爆炸还是梯度弥散，都是累乘的结果，lstm这边引入加法***

------------



##### 决策树

ID3,C4.5到CART，再随机森林，GBDT和XGBOOST

**id3:** （gain越大越好）

1. 对当前样本集合，计算所有属性的信息増益 

2. 选择信息増益最大的属性作为测试属性, 把测试属性取值相同的样本划为同一个子样本集

3. 若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。

   > 不足和缺点：
   >
   > * <u>*容易过拟合*</u>  ：对n个数据  有可能会分成100%纯洁的n组（比如对于时间日期 会容易做分裂）
   > * <u>*信息增益的缺点*</u>：信息增益准则对可取值数目较多的属性有所偏好 和过拟合的场景一致
   >
   > 改进方法
   >
   > <u>*避免过拟合*</u>： 剪枝
   >
   > <u>*信息增益的缺点*</u>：引进 信息增益比

**C4.5**：

使用了信息增益比(增益率)   越大越好

![截屏2020-03-01下午11.04.40](/Users/elvis/Library/Application Support/typora-user-images/截屏2020-03-01下午11.04.40.png)

属性 a 的可能取值数目越多（即 V 越大），则 IV (a）的值通常会越大。 （IV是属性a的固有值）

由于C4.5选择了增益率 所以它能处理 ID3不能处理的连续描述属性

**CART：**

使用了基尼指数（选择基尼指数小的属性）   能处理分类和回归
$$
\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
&=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}
\end{aligned}
$$


**<u>决策树的优劣：</u>**

- 优势：

  非黑盒 (知道内部怎么划分的)

  轻松去除无关 attribute (Gain=0) 

  Test 起来很快（O (depth）

- 劣势：

  只能线性分割数据

  贪婪算法（可能找不到最好的树）



---



##### 条件随机场

随机条件场的原理，对比了下条件随机场和隐马

##### 优化算法

##### Dropout

##### BN

##### 梯度下降

是一个多元函数的 全导数(每个维度下的偏导数组成)

##### bagging和boosting的区别

Bagging 是在每个bootstrap样本（对总样本有放回地随机均匀抽取N个样本）上分别训练小分类器(x1 x2 x3...)， 然后让x分类器进行投票（回归的话则是取均值）

bagging通过在不同数据集上训练模型降低分类器的方差。换句话说，bagging可以预防过拟合。bagging的有效性来自不同训练数据集上单独模型的不同，它们的误差在投票过程中相互抵消

极限状态下 假设当X_1和X_2完全独立时：
$$
\begin{aligned}
\operatorname{Var}\left(\frac{\sum X_{i}}{n}\right) &=\operatorname{Var}\left(\frac{X_{1}+X_{2}}{2}\right)=\frac{1}{4} \operatorname{Var}\left(X_{1}+X_{2}\right)=\frac{1}{4}\left(\operatorname{Var}\left(X_{1}\right)+\operatorname{Var}\left(X_{2}\right)\right)=\\
\frac{1}{4}\left(2 \operatorname{Var}\left(X_{1}\right)\right)=& \frac{\operatorname{Var}\left(X_{i}\right)}{2}=\frac{\operatorname{Var}\left(X_{i}\right)}{n}
\end{aligned}
$$
此时可以显著降低variance 方差



Boosting是将多个弱学习器组合成强学习器（能降低偏差）

1. 先在原数据集长出一个tree
2. 把之前tree没有完美分类的数据重新weight
3. 用新的re-weiight tree再训练出一个tree
4. 最终的分类结果由加权投票决定



Boosting中基模型按次序进行训练,而基模型的训练集按照某种策略每次都进行一定的转化,最后以一定的方式将基分类器组合成一个强分类器。

区别：

* Bagging的各个预测函数可以并行生成,Boosting的各预测函数只能顺序生成。

* Bagging中整体模型的期望近似于基模型的期望,所以整体模型的偏差相似于基模型的偏差,因此Bagging中的基模型为强模型(强模型拥有低偏差高方差)。

* Boosting中的基模型为弱模型,若不是弱模型会导致整体模型的方差很大。

------

##### 从方差和偏差角度比较bagging和boosting

bagging(随机森林) 是通过减少模型方差提高性能；

* bagging是说，各个基分类器是并行的，分类任务是vote，回归是mean

boosting(GBDT) 是通过减少模型偏差提高性能。

* boosting是串行的嘛，就相当于每次都在减少loss，偏差的定义刚好【是采样得到的样本训练出来的模型的输出的平均值，和真实模型输出的差值】

> 偏差(bias): 预测值与真实值之间的距离  偏差越大，越偏离真实数据
>
> 方差(variance)：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。 方差越大，数据的分布越分散

##### ![img](https://img-blog.csdn.net/20170505104127311?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDQ2NTYzOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 

----

##### 随机森林

每棵树看到的都是有限的  但是所有树加起来看到的是完整的样本和属性

不做任何剪枝

单个分类器C1：

- 有限样本量
- 有限属性类 



RF计算特征重要度采用的是基尼指数（Gini index）

频率选择、Gini重要度、排序重要度都可以用Gini index计算

高斯混合模型采用的是期望最大化（EM）算法

----

##### Adaboost：

https://zhuanlan.zhihu.com/p/39972832

1. 初始化权值分布

2. 进行多轮迭代  计算$$G_{m}(x)$$在训练数据集上的分类误差率   

   <u>(这里是**样本**权重)</u>
   $$
   \begin{array}{l}
   e_{m}=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) \\
   =\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)
   \end{array}
   $$

3. 计算$$G_{m}(x)$$系数

   <u>(这里是**分类器**权重 告诉分类器的重要程度)</u>
   $$
   \alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
   $$

4. 更新训练数据集权重（u是权重   错误则 *d；正确则 /d  ）

   <img src="/Users/elvis/ITProjects/GitHub/elvis_notebook/面试/截屏2020-03-22下午5.06.57.png" alt="截屏2020-03-22下午5.06.57" style="zoom:67%;" />

5. 最后组合各个弱分类器
   $$
   H(x)=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} f_{t}(x)\right)
   $$

---

##### GBDT:

> Adaboost的regression版本  是一个加法模型

1. 把adaboost的 $$e_{m}$$ 计算中的$$I()$$  变成类似mse的计算

2. 用残差作为下一轮的学习目标

3. 最终结果由加权和值得到（比如预测18岁，C1预测12岁，C2预测5岁，C3预测1岁）

   > X1 --> C1  比如X1 = 18  预测出来12
   >
   > X2 --> C2   X2 = 6   预测出来5
   >
   > X3 --> C3   X3 = 1
   >
   > GBDT中 这个X是残差；Adaboost是 reweight的train_data

**如何求解:**

用前向分布算法

![截屏2020-03-19下午4.28.10](/Users/elvis/ITProjects/GitHub/elvis_notebook/面试/截屏2020-03-19下午4.28.10.png)

##### XGBOOST:

[推导](https://zhuanlan.zhihu.com/p/92837676)



[详细理解原理](https://cloud.tencent.com/developer/article/1080182)

[具体面试问题](https://cloud.tencent.com/developer/article/1524927)

1. 使用了L1 L2正则化 防止overfitting 





----

##### LR和XGB算法做特征处理有什么区别？随机森林怎么进行特征选择？等特征处理方面相关的问题

* Xgboost：优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

  适用场景：分类回归问题都可以。

* Lr：优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

  缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

  不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

  适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

##### XGB和GDBT相比有什么优势

1. 使用L1 L2范式 防止过拟合
2. 对代价函数一阶和二阶求导，更快的 Converge 
3. 树长全后再从底部向上减枝，防止算法贪婪

##### LGB:

在xgb的基础进行进一步优化   减少了计算量同时一定程度上也起到了防止过拟合的作用：

* GOSS（梯度单边采样）：減少样本数量。
* EFB（特征绑定技术）：减少特征数量。
* Hist（直方图算法）：減少候选点数量。



##### Batch normalization(给nn训练加速)

BN被添加在全连接层和激活函数之间

![img](https://pic2.zhimg.com/80/v2-d3ccd01453f215cf3357192debd14489_720w.png)

和普通标准化一样，是将分散的数据统一的一种做法。

> 去量纲能让数据具有统一规格, 能让机器学习更容易学习到数据之中的规律.

在神经网络中, 数据分布对训练会产生影响。 如果用tanh或者sigmoid的激活函数(x = 0.1wx=0.1|x=20 wx=20)   那么容易产生因wx相乘过大而处于激活函数的饱和区    这样就容易丢弃消息    而批标准化使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分，使得数据能在每一个区间分布

![img](https://pic3.zhimg.com/80/v2-b31f7d863179f5f0b93d40c4fabbc31a_720w.png)

好处：

1. 加快学习速度
2. 有类似正则化给nn提供了一些噪音从而略微避免过拟合



##### 各种机器学习算法的应用场景分别是什么

[知乎](https://www.zhihu.com/question/26726794)



##### 哈希表

哈希表的原理，解决哈希冲突的方式，哈希函数的选择，常用的哈希函数

##### Linux命令

#### 具体笔试问题

* 某电商网站现在需要预测用户未来一周内购买哪些商品，请问：

  1. 可以使用哪些评价指标（至少写出两个）？

  2. 你会使用或构造哪些特征 （至少写出五个）？

  3. 现可供使用的模型有Logistic模型 和 GBDT(Gradient Boosting Decison Tree)模型，请简述这两个模型的原理，并比较这两个模型的特点。

  4. 训练模型后在线下的离线评价效果很好，但上线使用后发现效果极差，请分析可能的原因及解决方案

     Ans:

     1） 可以使用的评价指标：F1值，AUC值

     2） 构造三大类特征：User features，Item features， Cross features。

     - user features：用户历史交易中的ctr转化率；用户的点击购买时差；用户在网站上的浏览习惯等。
     - item features：商品在历史交易中的ctr转化率；商品的点击热度；商品的点击购买时差；商品在所属种类中的热度排名等。
     - cross features：用户对商品的浏览、购买等行为的计数统计；用户对商品的点击热度、购买热度排序；用户对商品种类的热度排序；用户之间相似度；相似度大的用户之间的商品购买统计等。

     3） Logistic模型是假设数据服从伯努利分布，采用极大似然估计法求参，然后用梯度下降的方法对参数进行优化，最后用求概率的方式对样本实现二分类。GBDT模型是对多棵决策树采用提升的思想，即每次迭代都是拟合上一棵树残差的近似值，实现分类或回归预测。

     - Logistic是线性分类模型，GBDT是非线性的model。
     - Logistic采用的是sigmoid损失函数，GBDT的回归用MSE，分类用对数或者指数损失。
     - Logistic对所有样本一视同仁，GBDT每一轮迭代都更加关注分错的样本。

     4） 可能原因是模型的泛化能力差，即模型发生了过拟合，解决方案：

     - 增加线下的样本规模；
     - 减少线下提取的特征数目；
     - 增加正则化项；

#### 数据挖掘和机器学习面试问题汇总

  1. 如何权衡偏差和方差？
2. 什么是梯度下降？
3. 解释一下过拟合和欠拟合，如何解决这两种问题？
4. 如何处理维度灾难？
5. 什么是正则化项。为什么要使用正则化，说出一些常用的正则化方法？
6. 讲解一下PCA原理
7. 为什么在神经网络中Relu激活函数会比Sigmoid激活函数用的更多？

8. 什么是数据标准化，为什么要进行数据标准化？

       我认为这个问题需要重视。数据标准化是预处理步骤，将数据标准化到一个特定的范围能够在反向传播中保证更好的收敛。一般来说，是将该值将去平均值后再除以标准差。如果不进行数据标准化，有些特征（值很大）将会对损失函数影响更大（就算这个特别大的特征只是改变了1%，但是他对损失函数的影响还是很大，并会使得其他值比较小的特征变得不重要了）。因此数据标准化可以使得每个特征的重要性更加均衡。

9. 解释什么是降维，在哪里会用到降维，它的好处是什么？

       降维是指通过保留一些比较重要的特征，去除一些冗余的特征，减少数据特征的维度。而特征的重要性取决于该特征能够表达多少数据集的信息，也取决于使用什么方法进行降维。而使用哪种降维方法则是通过反复的试验和每种方法在该数据集上的效果。一般情况会先使用线性的降维方法再使用非线性的降维方法，通过结果去判断哪种方法比较合适。而降维的好处是：
    （1）节省存储空间；
    （2）加速计算速度（比如在机器学习算法中），维度越少，计算量越少，并且能够使用那些不适合于高维度的算法；
    （3）去除一些冗余的特征，比如降维后使得数据不会既保存平方米和平方英里的表示地形大小的特征；
    （4）将数据维度降到2维或者3维使之能可视化，便于观察和挖掘信息。
    （5）特征太多或者太复杂会使得模型过拟合。

10. 如何处理缺失值数据？

       数据中可能会有缺失值，处理的方法有两种，一种是删除整行或者整列的数据，另一种则是使用其他值去填充这些缺失值。在Pandas库，有两种很有用的函数用于处理缺失值：isnull()和dropna()函数能帮助我们找到数据中的缺失值并且删除它们。如果你想用其他值去填充这些缺失值，则可以是用fillna()函数。

11. 解释聚类算法

      请参考（https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68--详细讲解各种聚类算法）

12. 你会如何进行探索性数据分析(EDA)？

       EDA的目的是去挖掘数据的一些重要信息。一般情况下会从粗到细的方式进行EDA探索。一开始我们可以去探索一些全局性的信息。观察一些不平衡的数据，计算一下各个类的方差和均值。看一下前几行数据的信息，包含什么特征等信息。使用Pandas中的df.info()去了解哪些特征是连续的，离散的，它们的类型(int、float、string)。接下来，删除一些不需要的列，这些列就是那些在分析和预测的过程中没有什么用的。

       比如：某些列的值很多都是相同的，或者这些列有很多缺失值。当然你也可以去用一些中位数等去填充这些缺失值。然后我们可以去做一些可视化。对于一些类别特征或者值比较少的可以使用条形图。类标和样本数的条形图。找到一些最一般的特征。对一些特征和类别的关系进行可视化去获得一些基本的信息。然后还可以可视化两个特征或三个特征之间的关系，探索特征之间的联系。

      你也可以使用PCA去了解哪些特征更加重要。组合特征去探索他们的关系，比如当A=0，B=0的类别是什么，A=1，B=0呢？比较特征的不同值，比如性别特征有男女两个取值，我们可以看下男和女两种取值的样本类标会不会不一样。

      另外，除了条形图、散点图等基本的画图方式外，也可以使用PDF\CDF或者覆盖图等。观察一些统计数据比如数据分布、p值等。这些分析后，最后就可以开始建模了。

       一开始可以使用一些比较简单的模型比如贝叶斯模型和逻辑斯谛回归模型。如果你发现你的数据是高度非线性的，你可以使用多项式回归、决策树或者SVM等。特征选择则可以基于这些特征在EDA过程中分析的重要性。如果你的数据量很大的话也可以使用神经网络。然后观察ROC曲线、查全率和查准率。

13. 你是怎么考虑使用哪些模型的？

       其实这个是有很多套路的。我写了一篇关于如何选择合适的回归模型，链接在这（https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef）。

14. 在图像处理中为什么要使用卷积神经网络而不是全连接网络？

       这个问题是我在面试一些视觉公司的时候遇到的。答案可以分为两个方面：首先，卷积过程是考虑到图像的局部特征，能够更加准确的抽取空间特征。如果使用全连接的话，我们可能会考虑到很多不相关的信息。其次，CNN有平移不变性，因为权值共享，图像平移了，卷积核还是可以识别出来，但是全连接则做不到。

15. 是什么使得CNN具有平移不变性？

       正如上面解释，每个卷积核都是一个特征探测器。所以就像我们在侦查一样东西的时候，不管物体在图像的哪个位置都能识别该物体。因为在卷积过程，我们使用卷积核在整张图片上进行滑动卷积，所以CNN具有平移不变性。

16. 为什么实现分类的CNN中需要进行Max-pooling？

       Max-pooling可以将特征维度变小，使得减小计算时间，同时，不会损失太多重要的信息，因为我们是保存最大值，这个最大值可以理解为该窗口下的最重要信息。同时，Max-pooling也对CNN具有平移不变性提供了很多理论支撑，详细可以看吴恩达的benefits of MaxPooling（https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers）。

17. 为什么应用于图像切割的CNN一般都具有Encoder-Decoder架构？

       Encoder CNN一般被认为是进行特征提取，而decoder部分则使用提取的特征信息并且通过decoder这些特征和将图像缩放到原始图像大小的方式去进行图像切割。

18. 什么是batch normalization，原理是什么？

       Batch Normalization就是在训练过程，每一层输入加一个标准化处理。深度神经网络之所以复杂有一个原因就是由于在训练的过程中上一层参数的更新使得每一层的输入一直在改变。所以有个办法就是去标准化每一层的输入。具体归一化的方式如下图，如果只将归一化的结果进行下一层的输入，这样可能会影响到本层学习的特征，因为可能该层学习到的特征分布可能并不是正态分布的，这样强制变成正态分布会有一定影响，所以还需要乘上γ和β，这两个参数是在训练过程学习的，这样可以保留学习到的特征。

来自网络
       神经网络其实就是一系列层组合成的，并且上一层的输出作为下层的输入，这意味着我们可以将神经网络的每一层都看成是以该层作为第一层的小型序列网络。这样我们在使用激活函数之前归一化该层的输出，然后将其作为下一层的输入，这样就可以解决输入一直改变的问题。

19. 为什么卷积核一般都是3*3而不是更大？

        这个问题在VGGNet模型中很好的解释了。主要有这2点原因：第一，相对于用较大的卷积核，使用多个较小的卷积核可以获得相同的感受野和能获得更多的特征信息，同时使用小的卷积核参数更少，计算量更小。第二：你可以使用更多的激活函数，有更多的非线性，使得在你的CNN模型中的判决函数有更有判决性。

20. 你有一些跟机器学习相关的项目吗？

       对于这个问题，你可以从你做过的研究与他们公司的业务之间的联系上作答。你所学到的技能是否有一些可能与他们公司的业务或你申请的职位有关？不需要是100％相吻合的，只要以某种方式相关就可以。这样有助于让他们认为你可以在这个职位上所产生的更大价值。

21. 解释一下你现在研究生期间的研究？平时都在做什么工作？未来的方向是什么？

      这些问题的答案都跟20题的回答思路是一致的。

#### 某独角兽公司数据挖掘工程师岗位 2000字面试总结

##### 一 面

**1**. 逻辑回归损失函数

（我的回答：对数损失，或者预测值与实际值的交叉熵损失）

**2**. 如果样本不均衡，需要在损失函数上做操作，该怎么处理？

（我的回答：这个之前没遇到过，不过处理样本均衡可以通过下采样和上采样等方法）

**2**. 伯努利分布概率为P，执行N次中k次发生，用极大似然估计估算P的值

（我的回答：在纸上写出了二项式分布的概率函数作为似然函数,P为参数，求argmax, （似然函数求导为零得到）P=k/N）

**3**. L1,L2正则化。L1正则化项怎么求导

（我的回答：L1,L2正则化主要为了防止过拟合，控制模型复杂度，使其具有更好的泛化能力。

L2正则化从贝叶斯理论看，参数分布服从高斯先验。L2范数是各个参数平方和，是每个wi^2 < C 的条件下最小化训练损失（经验损失）Loss，根据拉格朗日乘子法可以得到L = 训练损失 + 入L2范数；L2能起到压缩参数的作用

L正则化从贝叶斯理论看，参数分布服从拉普拉斯先验。L1范数是各个参数绝对值之和，是每个|wi| < C 的条件下最小化训练损失（经验损失）Loss，根据拉格朗日乘子法可以得到L = 训练损失 + 入L1范数；L1范数能起到是参数稀疏化作用，又是可以作为特征选择。



（当时感觉回答完这个问题，进二面没啥问题了）

**4**. 决策树算法以及防止过拟合方法。信息增益率比信息增益多了什么？

（我的回答：我先介绍了自信息，信息熵（自信息的期望），信息增益等相关概念。

基于信息增益的ID3，基于信息增益率的C4.5，基于基尼指数的CART等

剪枝：后剪枝，预剪枝（调节树的深度，叶子的样本个数等方式），随机森林Bagging

信息增益率相比于信息增益可以认为做了一个约束，除以原样本信息熵，避免用信息增益因类别过多一下子就全部分开，分成很矮，叶子很多的树（这里表达可能不太准确）。

）

**5**. 写出SVM损失函数，如果过拟合，怎么参数调整C

SVM损失函数: 01  损失函数  hinge  指数  对数损失

（我的回答：我写出了正则项，让他给点提示。他说合页损失，给我画了损失函数图并和ReLu做比较。C求和（yif(xi)）+ 1/2 ||w||,如果过拟合，应该减小C（这里如果除以C，那么参数就被整合到了正则项里面，可能更好理解）。还有如果因用到核函数过拟合，降低核函数复杂度）

C一般可以选择为：10^t , t=[- 4，4]就是0.0001 到10000。选择的越大，表示对错误例惩罚程度越大，可能会导致模型过拟合

**6**. 随机森林与boosting区别

（我的回答：他们都是集成学习方法，学习多个有差异且有一定精确度（>50%）的弱分类器。主要区别随机森林用到并行，boosting通常情况下串行）（这个不是很完整）

**7**. Xgboost在GBDT基础上做了哪些改进

（我的回答：Xgboost是陈天奇大神在GBDT基础上提出来的，

最大差别损失函数用到了二阶导数的信息。

在特征粒度裂解上用到了并行处理，加快速度。

特征裂解位置预先计算，做了排序并缓存，按百分位选择，而不是像GBDT全部遍历贪心选择。

用到的损失函数用到自定义的GINI系数。

同时基分类器不仅仅是回归树，可以是加了L1L2正则化的线性分类器等）

（在网上看了好多改进，说了这几个主要的）



##### 二 面

**1**.求sqrt()

**2**. 一个数组，求除了某元素自身位置之外的其他元素累积，生成一个同长度的数组。要求不能用除法，时间复杂度O（n），辅助空间O（1）

**3** 又问了SVM参数怎么调

**4** 又问了决策树怎么防止过拟合

**5** 问了L1，L2正则化的作用，为什么

**6**. 决策树算法：特征是连续值，特征是离散值的处理

**8** 趣味题，如果一把枪弹夹里有六个子弹，其中有两个子弹会打死人，四个不会有受伤。假如面试官打了自己一枪没死，现在把枪交给你，你得朝自己开一枪，问开枪之前，你是否会让弹夹旋转，切换子弹。







##### xgb股票预测：

- GBDT的原理 （理论基础）
- 决策树节点分裂时如何选择特征，写出Gini index和Information Gain的公式并举例说明（理论基础）
- 分类树和回归树的区别是什么？（理论基础）
- 与Random Forest作比较，并以此介绍什么是模型的Bias和Variance（理论基础）
- XGBoost的参数调优有哪些经验（工程能力）
- XGBoost的正则化是如何实现的（工程能力）
- XGBoost的并行化部分是如何实现的（工程能力）
- 为什么预测股票涨跌一般都会出现严重的过拟合现象（业务理解）
- 如果选用一种其他的模型替代XGBoost，你会选用什么？（业务理解和知识面）

除了上面的问题，我会再检查一下面试者对NN，RNN，个别聚类算法，模型评估等知识的理解程度以及对GAN，LSTM，online learning是否有基本理解，这是考察面试者对经典以及前沿的机器学习知识的了解程度。再稍微检查一下面试者对工具的了解程度，写一段简单的spark或者map reduce的程序，如果无误的话，那么可以说这位面试者的机器学习部分是完全合格的。



##### 什么是Map，什么是Reduce?

考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。Map阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似(hello, 12100次)，(world，15214次)等等这样的Pair(我这里把Map和Combine放在一起说以便简化);这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果(当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊)。然后这些Reducer将再次汇总，(hello，12100)+(hello，12311)+(hello，345881)= (hello，370292)。每个Reducer都如上处理，你就得到了整个文件的词频结果。