[toc]





#### SVM

1. 线性可分支持向量机(硬间隔支持向量机)
2. 线性支持向量机(软间隔支持向量机)
3. 非线性支持向量机(核函数+软间隔)

### 题目

#### 面试问题

> 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。——周志华《机器学习》

我的理解是mse是最小二乘法的实现

---------



##### 欠拟合和过拟合

> 欠拟合: **模型**没有很好地捕捉到数据特征，不能够很好地**拟合数据**  （模型无法得到较低的训练误差）

**解决方法**:

1. 添加其他特征项
2. 减少正则化参数

> 过拟合: 模型把数据学习的太彻底  模型**泛化能力**太差 （模型的训练误差远小于它在测试数据集上的误差）

**解决方法**:

1. 重新清洗数据(原因是数据不纯)
2. 增大训练数据的量(train size)
3. 采用正则化方法
4. nn里面用dropout

--------



##### 为什么正则化可以解决过拟合

正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。

**L2范数：**

对于下面的线性回归中
$$
\ell\left(w_{1}, w_{2}, b\right)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right)^{2}
$$
将权重参数用向量 𝑤=[𝑤1,𝑤2] 表示，带有 𝐿2 范数惩罚项的新损失函数为
$$
\ell\left(w_{1}, w_{2}, b\right)+\frac{\lambda}{2 n}|w|^{2}
$$

----



##### ml中的损失函数、期望风险、经验风险、结构风险

* 损失函数(cost func): 模型预测的值与真实值之间的差距
* 结构风险 (Structural risk): 可以看做对经验风险的一个优化;是对 经验风险 和期望风险的折中，在经验风险函数后面加一个正则化 项（惩罚项）, 得到结构分险。

> 期望风险与经验风险的区别：
> 期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。
>
> 经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。
>
> 缺点：
> 只考虑经验风险的话，会出现过度拟合现象，即模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办？这就需要结构风险。
>
> 链接：https://www.jianshu.com/p/073a00d69acf

--------



##### 在进行线性回归时，为什么最小二乘法是最优方法？

为什么用欧式距离作为误差度量 （即MSE）

它简单。便于计算。通常所推导得到的问题是凸问题，具有对称性，可导性。通常具有解析解，此外便于通过迭代的方式求解。

缺点: 

误差信号和原信号无关。只要误差信号不变，无论原信号如何，MSE均不变。例如，对于固定误差[1 1 1]，无论加在[1 2 3]产生[2 3 4]还是加在[0 0 0]产生[1 1 1]，MSE的计算结果不变。

无法比较空间 时间上的差别

------



##### lr手推 原理、优缺点以及应用场景(LR是什么假设，损失函数是怎么回事，怎样更新参数什么的)

> lr和线性回归的关系:
>
>  线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

损失函数：交叉熵(cross entropy)
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
$$

--------



##### svm原理

----------



##### 最常用的5个回归损失函数

>  (from https://www.jiqizhixin.com/articles/2018-06-21-3)

* 均方误差(L2损失)
  $$
  M S E=\sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}
  $$

* 平均绝对值误差（也称L1损失）

  > MAE对异常点有更好的鲁棒性  
  >
  > 一些情况下L2损失会因为存在一个异常点，而导致误差非常大。

  $$
  M A E=\sum_{i=1}^{n}\left|y_{i}-y_{i}^{p}\right|
  $$

* Log-Cosh损失

  > Log-cosh是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。

-------------



##### LSTM为什么可以解决梯度弥散的问题

RNN由于网络较深,后面层的输出误差很难影响到前面层的计算,RNN的某一单元主要受它附近单元的影响。而LSTM因为可以通过阀门记忆一些长期的信息,相应的也就保留了更多的梯度



RNN中，每个记忆单元h_t-1都会乘上一个W和激活函数的导数，这种连乘使得记忆衰减的很快，而LSTM是通过记忆和当前输入"相加"，使得之前的记忆会继续存在而不是受到乘法的影响而部分“消失”，因此不会衰减。但是这种naive的做法太直白了，实际上就是个线性模型，在学习效果上不够好，因此LSTM引入了那3个门
————————————————
版权声明：本文为CSDN博主「白痴一只」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/hx14301009/article/details/80401227

***不管是梯度爆炸还是梯度弥散，都是累乘的结果，lstm这边引入加法***

------------



##### 决策树

ID3,C4.5到CART，再随机森林，GBDT和XGBOOST

**id3:** （gain越大越好）

1. 对当前样本集合，计算所有属性的信息増益 

2. 选择信息増益最大的属性作为测试属性, 把测试属性取值相同的样本划为同一个子样本集

3. 若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。

   > 不足和缺点：
   >
   > * <u>*容易过拟合*</u>  ：对n个数据  有可能会分成100%纯洁的n组（比如对于时间日期 会容易做分裂）
   > * <u>*信息增益的缺点*</u>：信息增益准则对可取值数目较多的属性有所偏好 和过拟合的场景一致
   >
   > 改进方法
   >
   > <u>*避免过拟合*</u>： 剪枝
   >
   > <u>*信息增益的缺点*</u>：引进 信息增益比

**C4.5**：

使用了信息增益比(增益率)   越大越好

![截屏2020-03-01下午11.04.40](/Users/elvis/Library/Application Support/typora-user-images/截屏2020-03-01下午11.04.40.png)

属性 a 的可能取值数目越多（即 V 越大），则 IV (a）的值通常会越大。 （IV是属性a的固有值）

由于C4.5选择了增益率 所以它能处理 ID3不能处理的连续描述属性

**CART：**

使用了基尼指数（选择基尼指数小的属性）   能处理分类和回归
$$
\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
&=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}
\end{aligned}
$$


**<u>决策树的优劣：</u>**

- 优势：

  非黑盒 (知道内部怎么划分的)

  轻松去除无关 attribute (Gain=0) 

  Test 起来很快（O (depth）

- 劣势：

  只能线性分割数据

  贪婪算法（可能找不到最好的树）



---



##### 条件随机场

随机条件场的原理，对比了下条件随机场和隐马

##### 优化算法

##### Dropout

##### BN

##### bagging和boosting的区别

Bagging 是在每个bootstrap样本（对总样本有放回地随机均匀抽取N个样本）上分别训练小分类器(x1 x2 x3...)， 然后让x分类器进行投票（回归的话则是取均值）

bagging通过在不同数据集上训练模型降低分类器的方差。换句话说，bagging可以预防过拟合。bagging的有效性来自不同训练数据集上单独模型的不同，它们的误差在投票过程中相互抵消

极限状态下 假设当X_1和X_2完全独立时：
$$
\begin{aligned}
\operatorname{Var}\left(\frac{\sum X_{i}}{n}\right) &=\operatorname{Var}\left(\frac{X_{1}+X_{2}}{2}\right)=\frac{1}{4} \operatorname{Var}\left(X_{1}+X_{2}\right)=\frac{1}{4}\left(\operatorname{Var}\left(X_{1}\right)+\operatorname{Var}\left(X_{2}\right)\right)=\\
\frac{1}{4}\left(2 \operatorname{Var}\left(X_{1}\right)\right)=& \frac{\operatorname{Var}\left(X_{i}\right)}{2}=\frac{\operatorname{Var}\left(X_{i}\right)}{n}
\end{aligned}
$$
此时可以显著降低variance 方差



Boosting是将多个弱学习器组合成强学习器（能降低偏差）

1. 先在原数据集长出一个tree
2. 把之前tree没有完美分类的数据重新weight
3. 用新的re-weiight tree再训练出一个tree
4. 最终的分类结果由加权投票决定



Boosting中基模型按次序进行训练,而基模型的训练集按照某种策略每次都进行一定的转化,最后以一定的方式将基分类器组合成一个强分类器。

区别：

* Bagging的各个预测函数可以并行生成,Boosting的各预测函数只能顺序生成。

* Bagging中整体模型的期望近似于基模型的期望,所以整体模型的偏差相似于基模型的偏差,因此Bagging中的基模型为强模型(强模型拥有低偏差高方差)。

* Boosting中的基模型为弱模型,若不是弱模型会导致整体模型的方差很大。

------

##### 从方差和偏差角度比较bagging和boosting

bagging(随机森林) 是通过减少模型方差提高性能；

* bagging是说，各个基分类器是并行的，分类任务是vote，回归是mean

boosting(GBDT) 是通过减少模型偏差提高性能。

* boosting是串行的嘛，就相当于每次都在减少loss，偏差的定义刚好【是采样得到的样本训练出来的模型的输出的平均值，和真实模型输出的差值】

> 偏差(bias): 预测值与真实值之间的距离  偏差越大，越偏离真实数据
>
> 方差(variance)：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。 方差越大，数据的分布越分散

##### ![img](https://img-blog.csdn.net/20170505104127311?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDQ2NTYzOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 

----

##### 随机森林

每棵树看到的都是有限的  但是所有树加起来看到的是完整的样本和属性

不做任何剪枝

单个分类器C1：

- 有限样本量
- 有限属性类 



RF计算特征重要度采用的是基尼指数（Gini index）

频率选择、Gini重要度、排序重要度都可以用Gini index计算

高斯混合模型采用的是期望最大化（EM）算法

----

##### Adaboost：

https://zhuanlan.zhihu.com/p/39972832

1. 初始化权值分布

2. 进行多轮迭代  计算$$G_{m}(x)$$在训练数据集上的分类误差率   

   <u>(这里是**样本**权重)</u>
   $$
   \begin{array}{l}
   e_{m}=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) \\
   =\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)
   \end{array}
   $$

3. 计算$$G_{m}(x)$$系数

   <u>(这里是**分类器**权重 告诉分类器的重要程度)</u>
   $$
   \alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
   $$

4. 更新训练数据集权重
5. 最后组合各个弱分类器

---

##### GBDT:

> Adaboost的regression版本

1. 把adaboost的 $$e_{m}$$ 计算中的$$I()$$  变成类似mse的计算

2. 用残差作为下一轮的学习目标

3. 最终结果由加权和值得到（比如预测18岁，C1预测12岁，C2预测5岁，C3预测1岁）

   > X1 --> C1  比如X1 = 18  预测出来12
   >
   > X2 --> C2   X2 = 6   预测出来5
   >
   > X3 --> C3   X3 = 1
   >
   > GBDT中 这个X是残差；Adaboost是 reweight的train_data

##### XGBOOST:

1. 使用了L1 L2正则化 防止overfitting 





----

##### LR和XGB算法做特征处理有什么区别？随机森林怎么进行特征选择？等特征处理方面相关的问题

* Xgboost：优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

  适用场景：分类回归问题都可以。

* Lr：优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

  缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

  不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

  适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

##### XGB和GDBT相比有什么优势

1. 使用L1 L2范式 防止过拟合
2. 对代价函数一阶和二阶求导，更快的 Converge 
3. 树长全后再从底部向上减枝，防止算法贪婪



##### Batch normalization



##### 各种机器学习算法的应用场景分别是什么

[知乎](https://www.zhihu.com/question/26726794)



##### 哈希表

哈希表的原理，解决哈希冲突的方式，哈希函数的选择，常用的哈希函数

##### Linux命令

#### 具体笔试问题

* 某电商网站现在需要预测用户未来一周内购买哪些商品，请问：

  1. 可以使用哪些评价指标（至少写出两个）？

  2. 你会使用或构造哪些特征 （至少写出五个）？

  3. 现可供使用的模型有Logistic模型 和 GBDT(Gradient Boosting Decison Tree)模型，请简述这两个模型的原理，并比较这两个模型的特点。

  4. 训练模型后在线下的离线评价效果很好，但上线使用后发现效果极差，请分析可能的原因及解决方案

     Ans:

     1） 可以使用的评价指标：F1值，AUC值

     2） 构造三大类特征：User features，Item features， Cross features。

     - user features：用户历史交易中的ctr转化率；用户的点击购买时差；用户在网站上的浏览习惯等。
     - item features：商品在历史交易中的ctr转化率；商品的点击热度；商品的点击购买时差；商品在所属种类中的热度排名等。
     - cross features：用户对商品的浏览、购买等行为的计数统计；用户对商品的点击热度、购买热度排序；用户对商品种类的热度排序；用户之间相似度；相似度大的用户之间的商品购买统计等。

     3） Logistic模型是假设数据服从伯努利分布，采用极大似然估计法求参，然后用梯度下降的方法对参数进行优化，最后用求概率的方式对样本实现二分类。GBDT模型是对多棵决策树采用提升的思想，即每次迭代都是拟合上一棵树残差的近似值，实现分类或回归预测。

     - Logistic是线性分类模型，GBDT是非线性的model。
     - Logistic采用的是sigmoid损失函数，GBDT的回归用MSE，分类用对数或者指数损失。
     - Logistic对所有样本一视同仁，GBDT每一轮迭代都更加关注分错的样本。

     4） 可能原因是模型的泛化能力差，即模型发生了过拟合，解决方案：

     - 增加线下的样本规模；
     - 减少线下提取的特征数目；
     - 增加正则化项；

#### 数据挖掘和机器学习面试问题汇总

  1. 如何权衡偏差和方差？
2. 什么是梯度下降？
3. 解释一下过拟合和欠拟合，如何解决这两种问题？
4. 如何处理维度灾难？
5. 什么是正则化项。为什么要使用正则化，说出一些常用的正则化方法？
6. 讲解一下PCA原理
7. 为什么在神经网络中Relu激活函数会比Sigmoid激活函数用的更多？

8. 什么是数据标准化，为什么要进行数据标准化？

       我认为这个问题需要重视。数据标准化是预处理步骤，将数据标准化到一个特定的范围能够在反向传播中保证更好的收敛。一般来说，是将该值将去平均值后再除以标准差。如果不进行数据标准化，有些特征（值很大）将会对损失函数影响更大（就算这个特别大的特征只是改变了1%，但是他对损失函数的影响还是很大，并会使得其他值比较小的特征变得不重要了）。因此数据标准化可以使得每个特征的重要性更加均衡。

9. 解释什么是降维，在哪里会用到降维，它的好处是什么？

       降维是指通过保留一些比较重要的特征，去除一些冗余的特征，减少数据特征的维度。而特征的重要性取决于该特征能够表达多少数据集的信息，也取决于使用什么方法进行降维。而使用哪种降维方法则是通过反复的试验和每种方法在该数据集上的效果。一般情况会先使用线性的降维方法再使用非线性的降维方法，通过结果去判断哪种方法比较合适。而降维的好处是：
    （1）节省存储空间；
    （2）加速计算速度（比如在机器学习算法中），维度越少，计算量越少，并且能够使用那些不适合于高维度的算法；
    （3）去除一些冗余的特征，比如降维后使得数据不会既保存平方米和平方英里的表示地形大小的特征；
    （4）将数据维度降到2维或者3维使之能可视化，便于观察和挖掘信息。
    （5）特征太多或者太复杂会使得模型过拟合。

10. 如何处理缺失值数据？

       数据中可能会有缺失值，处理的方法有两种，一种是删除整行或者整列的数据，另一种则是使用其他值去填充这些缺失值。在Pandas库，有两种很有用的函数用于处理缺失值：isnull()和dropna()函数能帮助我们找到数据中的缺失值并且删除它们。如果你想用其他值去填充这些缺失值，则可以是用fillna()函数。

11. 解释聚类算法

      请参考（https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68--详细讲解各种聚类算法）

12. 你会如何进行探索性数据分析(EDA)？

       EDA的目的是去挖掘数据的一些重要信息。一般情况下会从粗到细的方式进行EDA探索。一开始我们可以去探索一些全局性的信息。观察一些不平衡的数据，计算一下各个类的方差和均值。看一下前几行数据的信息，包含什么特征等信息。使用Pandas中的df.info()去了解哪些特征是连续的，离散的，它们的类型(int、float、string)。接下来，删除一些不需要的列，这些列就是那些在分析和预测的过程中没有什么用的。

       比如：某些列的值很多都是相同的，或者这些列有很多缺失值。当然你也可以去用一些中位数等去填充这些缺失值。然后我们可以去做一些可视化。对于一些类别特征或者值比较少的可以使用条形图。类标和样本数的条形图。找到一些最一般的特征。对一些特征和类别的关系进行可视化去获得一些基本的信息。然后还可以可视化两个特征或三个特征之间的关系，探索特征之间的联系。

      你也可以使用PCA去了解哪些特征更加重要。组合特征去探索他们的关系，比如当A=0，B=0的类别是什么，A=1，B=0呢？比较特征的不同值，比如性别特征有男女两个取值，我们可以看下男和女两种取值的样本类标会不会不一样。

      另外，除了条形图、散点图等基本的画图方式外，也可以使用PDF\CDF或者覆盖图等。观察一些统计数据比如数据分布、p值等。这些分析后，最后就可以开始建模了。

       一开始可以使用一些比较简单的模型比如贝叶斯模型和逻辑斯谛回归模型。如果你发现你的数据是高度非线性的，你可以使用多项式回归、决策树或者SVM等。特征选择则可以基于这些特征在EDA过程中分析的重要性。如果你的数据量很大的话也可以使用神经网络。然后观察ROC曲线、查全率和查准率。

13. 你是怎么考虑使用哪些模型的？

       其实这个是有很多套路的。我写了一篇关于如何选择合适的回归模型，链接在这（https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef）。

14. 在图像处理中为什么要使用卷积神经网络而不是全连接网络？

       这个问题是我在面试一些视觉公司的时候遇到的。答案可以分为两个方面：首先，卷积过程是考虑到图像的局部特征，能够更加准确的抽取空间特征。如果使用全连接的话，我们可能会考虑到很多不相关的信息。其次，CNN有平移不变性，因为权值共享，图像平移了，卷积核还是可以识别出来，但是全连接则做不到。

15. 是什么使得CNN具有平移不变性？

       正如上面解释，每个卷积核都是一个特征探测器。所以就像我们在侦查一样东西的时候，不管物体在图像的哪个位置都能识别该物体。因为在卷积过程，我们使用卷积核在整张图片上进行滑动卷积，所以CNN具有平移不变性。

16. 为什么实现分类的CNN中需要进行Max-pooling？

       Max-pooling可以将特征维度变小，使得减小计算时间，同时，不会损失太多重要的信息，因为我们是保存最大值，这个最大值可以理解为该窗口下的最重要信息。同时，Max-pooling也对CNN具有平移不变性提供了很多理论支撑，详细可以看吴恩达的benefits of MaxPooling（https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers）。

17. 为什么应用于图像切割的CNN一般都具有Encoder-Decoder架构？

       Encoder CNN一般被认为是进行特征提取，而decoder部分则使用提取的特征信息并且通过decoder这些特征和将图像缩放到原始图像大小的方式去进行图像切割。

18. 什么是batch normalization，原理是什么？

       Batch Normalization就是在训练过程，每一层输入加一个标准化处理。深度神经网络之所以复杂有一个原因就是由于在训练的过程中上一层参数的更新使得每一层的输入一直在改变。所以有个办法就是去标准化每一层的输入。具体归一化的方式如下图，如果只将归一化的结果进行下一层的输入，这样可能会影响到本层学习的特征，因为可能该层学习到的特征分布可能并不是正态分布的，这样强制变成正态分布会有一定影响，所以还需要乘上γ和β，这两个参数是在训练过程学习的，这样可以保留学习到的特征。

来自网络
       神经网络其实就是一系列层组合成的，并且上一层的输出作为下层的输入，这意味着我们可以将神经网络的每一层都看成是以该层作为第一层的小型序列网络。这样我们在使用激活函数之前归一化该层的输出，然后将其作为下一层的输入，这样就可以解决输入一直改变的问题。

19. 为什么卷积核一般都是3*3而不是更大？

        这个问题在VGGNet模型中很好的解释了。主要有这2点原因：第一，相对于用较大的卷积核，使用多个较小的卷积核可以获得相同的感受野和能获得更多的特征信息，同时使用小的卷积核参数更少，计算量更小。第二：你可以使用更多的激活函数，有更多的非线性，使得在你的CNN模型中的判决函数有更有判决性。

20. 你有一些跟机器学习相关的项目吗？

       对于这个问题，你可以从你做过的研究与他们公司的业务之间的联系上作答。你所学到的技能是否有一些可能与他们公司的业务或你申请的职位有关？不需要是100％相吻合的，只要以某种方式相关就可以。这样有助于让他们认为你可以在这个职位上所产生的更大价值。

21. 解释一下你现在研究生期间的研究？平时都在做什么工作？未来的方向是什么？

      这些问题的答案都跟20题的回答思路是一致的。

#### 某独角兽公司数据挖掘工程师岗位 2000字面试总结

##### 一 面

**1**. 逻辑回归损失函数

（我的回答：对数损失，或者预测值与实际值的交叉熵损失）

**2**. 如果样本不均衡，需要在损失函数上做操作，该怎么处理？

（我的回答：这个之前没遇到过，不过处理样本均衡可以通过下采样和上采样等方法）

**2**. 伯努利分布概率为P，执行N次中k次发生，用极大似然估计估算P的值

（我的回答：在纸上写出了二项式分布的概率函数作为似然函数,P为参数，求argmax, （似然函数求导为零得到）P=k/N）

**3**. L1,L2正则化。L1正则化项怎么求导

（我的回答：L1,L2正则化主要为了防止过拟合，控制模型复杂度，使其具有更好的泛化能力。

L2正则化从贝叶斯理论看，参数分布服从高斯先验。L2范数是各个参数平方和，是每个wi^2 < C 的条件下最小化训练损失（经验损失）Loss，根据拉格朗日乘子法可以得到L = 训练损失 + 入L2范数；L2能起到压缩参数的作用

L2正则化从贝叶斯理论看，参数分布服从拉普拉斯先验。L1范数是各个参数绝对值之和，是每个|wi| < C 的条件下最小化训练损失（经验损失）Loss，根据拉格朗日乘子法可以得到L = 训练损失 + 入L1范数；L1范数能起到是参数稀疏化作用，又是可以作为特征选择。

同时，L0正则化是真正起到稀疏化作用，可以认为L1正则化是L0正则化的近似，因为L0正则化是NP问题，可能是非凸的，而且不易求导比较难用凸优化求解。

L1正则化在0点出不可导，一般做近似平滑处理，其他点得到W的符号）

（当时感觉回答完这个问题，进二面没啥问题了）

**4**. 决策树算法以及防止过拟合方法。信息增益率比信息增益多了什么？

（我的回答：我先介绍了自信息，信息熵（自信息的期望），信息增益等相关概念。

基于信息增益的ID3，基于信息增益率的C4.5，基于基尼指数的CART等

剪枝：后剪枝，预剪枝（调节树的深度，叶子的样本个数等方式），随机森林Bagging

信息增益率相比于信息增益可以认为做了一个约束，除以原样本信息熵，避免用信息增益因类别过多一下子就全部分开，分成很矮，叶子很多的树（这里表达可能不太准确）。

）

**5**. 写出SVM损失函数，如果过拟合，怎么参数调整C

（我的回答：我写出了正则项，让他给点提示。他说合页损失，给我画了损失函数图并和LeRu做比较。C求和（yif(xi)）+ 1/2 ||w||,如果过拟合，应该减小C（这里如果除以C，那么参数就被整合到了正则项里面，可能更好理解）。还有如果因用到核函数过拟合，降低核函数复杂度）

**6**. 随机森林与boosting区别

（我的回答：他们都是集成学习方法，学习多个有差异且有一定精确度（>50%）的弱分类器。主要区别随机森林用到并行，boosting通常情况下串行）（这个不是很完整）

**7**. Xgboost在GBDT基础上做了哪些改进

（我的回答：Xgboost是陈天奇大神在GBDT基础上提出来的，

最大差别损失函数用到了二阶导数的信息。

在特征粒度裂解上用到了并行处理，加快速度。

特征裂解位置预先计算，做了排序并缓存，按百分位选择，而不是像GBDT全部遍历贪心选择。

用到的损失函数用到自定义的GINI系数。

同时基分类器不仅仅是回归树，可以是加了L1L2正则化的线性分类器等）

（在网上看了好多改进，说了这几个主要的）



##### 二 面

**1**.求sqrt()

**2**. 一个数组，求除了某元素自身位置之外的其他元素累积，生成一个同长度的数组。要求不能用除法，时间复杂度O（n），辅助空间O（1）

**3** 又问了SVM参数怎么调

**4** 又问了决策树怎么防止过拟合

**5** 问了L1，L2正则化的作用，为什么

**6**. 决策树算法：特征是连续值，特征是离散值的处理

**8** 趣味题，如果一把枪弹夹里有六个子弹，其中有两个子弹会打死人，四个不会有受伤。假如面试官打了自己一枪没死，现在把枪交给你，你得朝自己开一枪，问开枪之前，你是否会让弹夹旋转，切换子弹。



##### 三 面

就问了一个问题:

如何用int_8位整型数据，模拟两个int_32位整型数据相乘（写代码实现）

(给了许多提示还是没写出来，挂了。)