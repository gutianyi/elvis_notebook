[toc]

#### 随机森林

* RF计算特征重要度采用的是基尼指数（Gini index）

  频率选择、Gini重要度、排序重要度都可以用Gini index计算

  高斯混合模型采用的是期望最大化（EM）算法

#### SVM

1. 线性可分支持向量机(硬间隔支持向量机)
2. 线性支持向量机(软间隔支持向量机)
3. 非线性支持向量机(核函数+软间隔)

### 题目

#### 面试问题

> 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。——周志华《机器学习》

我的理解是mse是最小二乘法的实现

##### ml中的损失函数、期望风险、经验风险、结构风险

* 损失函数(cost func): 模型预测的值与真实值之间的差距
* 结构风险 (Structural risk): 可以看做对经验风险的一个优化;是对 经验风险 和期望风险的折中，在经验风险函数后面加一个正则化 项（惩罚项）, 得到结构分险。

> 期望风险与经验风险的区别：
> 期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。
>
> 经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。
>
> 缺点：
> 只考虑经验风险的话，会出现过度拟合现象，即模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办？这就需要结构风险。
>
> 链接：https://www.jianshu.com/p/073a00d69acf



##### 在进行线性回归时，为什么最小二乘法是最优方法？

为什么用欧式距离作为误差度量 （即MSE）

它简单。便于计算。通常所推导得到的问题是凸问题，具有对称性，可导性。通常具有解析解，此外便于通过迭代的方式求解。

缺点: 

误差信号和原信号无关。只要误差信号不变，无论原信号如何，MSE均不变。例如，对于固定误差[1 1 1]，无论加在[1 2 3]产生[2 3 4]还是加在[0 0 0]产生[1 1 1]，MSE的计算结果不变。

无法比较空间 时间上的差别

##### lr手推 原理、优缺点以及应用场景

LR是什么假设，损失函数是怎么回事，怎样更新参数什么的

> lr和线性回归的关系:
>
>  线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

##### svm原理

##### 最常用的5个回归损失函数

>  (from https://www.jiqizhixin.com/articles/2018-06-21-3)

* 均方误差(L2损失)
  $$
  M S E=\sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}
  $$

* 平均绝对值误差（也称L1损失）

  > MAE对异常点有更好的鲁棒性  
  >
  > 一些情况下L2损失会因为存在一个异常点，而导致误差非常大。

  $$
  M A E=\sum_{i=1}^{n}\left|y_{i}-y_{i}^{p}\right|
  $$

* Log-Cosh损失

  > Log-cosh是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。



##### LSTM为什么可以解决梯度弥散的问题

RNN由于网络较深,后面层的输出误差很难影响到前面层的计算,RNN的某一单元主要受它附近单元的影响。而LSTM因为可以通过阀门记忆一些长期的信息,相应的也就保留了更多的梯度



RNN中，每个记忆单元h_t-1都会乘上一个W和激活函数的导数，这种连乘使得记忆衰减的很快，而LSTM是通过记忆和当前输入"相加"，使得之前的记忆会继续存在而不是受到乘法的影响而部分“消失”，因此不会衰减。但是这种naive的做法太直白了，实际上就是个线性模型，在学习效果上不够好，因此LSTM引入了那3个门
————————————————
版权声明：本文为CSDN博主「白痴一只」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/hx14301009/article/details/80401227

***不管是梯度爆炸还是梯度弥散，都是累乘的结果，lstm这边引入加法***

##### 决策树

ID3,C4.5到CART，再随机森林，GBDT和XGBOOST

##### 条件随机场

随机条件场的原理，对比了下条件随机场和隐马

##### 优化算法

##### Dropout

##### BN

##### XGBOOST原理、优缺点以及应用场景

##### 随机森林原理、优缺点以及应用场景

##### LR和XGB算法做特征处理有什么区别？随机森林怎么进行特征选择？等特征处理方面相关的问题

* Xgboost：优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

  适用场景：分类回归问题都可以。

* Lr：优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

  缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

  不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

  适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

##### XGB和GDBT相比有什么优势

##### 基础问了欠拟合和过拟合，从方差和偏差角度比较bagging和boosting

#####  

##### 哈希表

哈希表的原理，解决哈希冲突的方式，哈希函数的选择，常用的哈希函数

##### Linux命令

#### 具体笔试问题

* 某电商网站现在需要预测用户未来一周内购买哪些商品，请问：

  1. 可以使用哪些评价指标（至少写出两个）？

  2. 你会使用或构造哪些特征 （至少写出五个）？

  3. 现可供使用的模型有Logistic模型 和 GBDT(Gradient Boosting Decison Tree)模型，请简述这两个模型的原理，并比较这两个模型的特点。

  4. 训练模型后在线下的离线评价效果很好，但上线使用后发现效果极差，请分析可能的原因及解决方案

     Ans:

     1） 可以使用的评价指标：F1值，AUC值

     2） 构造三大类特征：User features，Item features， Cross features。

     - user features：用户历史交易中的ctr转化率；用户的点击购买时差；用户在网站上的浏览习惯等。
     - item features：商品在历史交易中的ctr转化率；商品的点击热度；商品的点击购买时差；商品在所属种类中的热度排名等。
     - cross features：用户对商品的浏览、购买等行为的计数统计；用户对商品的点击热度、购买热度排序；用户对商品种类的热度排序；用户之间相似度；相似度大的用户之间的商品购买统计等。

     3） Logistic模型是假设数据服从伯努利分布，采用极大似然估计法求参，然后用梯度下降的方法对参数进行优化，最后用求概率的方式对样本实现二分类。GBDT模型是对多棵决策树采用提升的思想，即每次迭代都是拟合上一棵树残差的近似值，实现分类或回归预测。

     - Logistic是线性分类模型，GBDT是非线性的model。
     - Logistic采用的是sigmoid损失函数，GBDT的回归用MSE，分类用对数或者指数损失。
     - Logistic对所有样本一视同仁，GBDT每一轮迭代都更加关注分错的样本。

     4） 可能原因是模型的泛化能力差，即模型发生了过拟合，解决方案：

     - 增加线下的样本规模；
     - 减少线下提取的特征数目；
     - 增加正则化项；