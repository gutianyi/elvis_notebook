[TOC]



### EDA

#### 绘图方法

##### 直方图

in order to 观察数据分布

##### 密度曲线图

概率密度函数  

##### 箱型图

便于查看数据的异常状况 以及不同数据间分布的对比

##### 小提琴图

可以看出某个值附近的分布频率  进阶版箱型图

#### 量化方法

##### 相关性分析

定序： 教育程度5 > 1 但是5-1=4是没有意义的

定距：商品价格

![截屏2020-04-21下午4.55.57](/Users/elvis/ITProjects/GitHub/elvis_notebook/比赛/截屏2020-04-21下午4.55.57.png)

##### 独立性分析  存在非线性关联

MVtest

![截屏2020-04-21下午7.42.08](/Users/elvis/ITProjects/GitHub/elvis_notebook/比赛/截屏2020-04-21下午7.42.08.png)

### 具体问题

#### 数据预处理过程中，对缺失值和不合理的值都有哪些比较好的处理方法?如何选择最合适的？

1.【智浪淘沙】**大神解答**

缺失值和不合理值，都置换为NAN后 ，可以构建决策树进行填充，过程是将没有缺失的其它特征作为特征输入，label 为要预测为空值的一类。用构建的模型对缺失值进行预测填充。

在填充方法里面 ，还有一种名为插值法的技术，pandas自带的插值法，有回归插值，拉格朗日插值,等等。

当然这些都是一些技巧，小数据集里面可能都没有直接置为NAN 用lgb训练来的方便。



2.【snail】**大神解答**

a) 用回归模型(根据已有的相关数据作为特征），拟合为一条线，代入特征求值，填充（时序常用）

b) 聚类（相邻窗口也行）后各个块的mean median填充

2） 离散数据

a) 无监督（各种聚类）都可以尝试一下

b) 分别对缺失值进行建模（推荐系统还是常用）

3) 类别少，缺失值大于70%直接删除

4) 异常值直接填为空，重复1,2步骤处理。或者直接删除（考虑列和行的缺失值和异常值（iqr判断）比列）

5) 离散值使用mode，连续值使用mean。时序值使用median（经验）



**3.【风度】大神解答**

用统计学的方法也可以，±3个西格玛，合格品率为99.73%，在这个数字之外的基本属于异常值，可以用剔除或者用99.73%这个值来填充。



**4.【zychyz】大神解答**

缺失值填充的话还可以采用前向/后向填充/插值（对时序型数据比较有用），如果采用填充中位数、均值的话要注意数据分布的改变。

#### 为什么在数据预处理时要对长尾分布进行log化为正态分布？

> Pearson相关分析（也称皮尔逊相关分析，很多时候直接称呼为相关分析），在实际研究中使用最多。其实Pearson相关分析也有着默认的前提条件，即数据满足正态分布性。但现实中的数据很难满足正态分布性，此时建议使用Spearman（斯皮尔曼）相关系数进行研究即可。

**1.【林有夕】大神解答**



1）对于树模型而言，关注的是树值特征的相对顺序，所以log之类的单调性不变的处理，对于特征而言是没有意义的。

2）对于搜索词这类的频率呈长尾分布的稀疏特征，对编码本身做log 也没有意义，因为编码表示的是类别含义，而非数值本身。

3）如果是对于label 做log变换，这在label呈长尾分布的回归题中常见。一方面可以减少模型过度学习边缘样本，另一方面，如果使用的是mae 或者smape 此类对于边缘值相对不敏感的指标的时候，对label 做log 变换就是一种很有效的处理。



**2.【鱼遇雨欲语与余】大神解答**



对长尾分布进行log化为正态分布的目的是它能够让它符合我们所做的假设，使我们能够在已有理论上对其分析。



简单来说可以总结如下三点：

1）研究的自变量数量级不一致时，取对数可消除这种数量级相差很大的情况。

2）取对数可以消除异方差。

3）取对数可以使非线性的变量关系转化为线性关系，更方便做参数估计。



**3.【TJB】大神解答**



我的理解是因为ln(1/x) = -ln(x) ,取对数可以把原来和1比较的数变成关于0对称，因此会更加“正态”,在经典线性回归模型假设中随机误差项是同方差的，对长尾分布数据取对数有助于减弱异方差，使回归系数更可靠。



但是如果我没想错，对于树模型来说，给变量取对数对模型是没有影响的，取不取对数是一样的。



**4.【OBM_Cover】大神解答**



在预测回归分析问题中，长尾分布的label（全是正数）可能会导致模型对测试集预测出负数结果。如果对label进行log后就可以至少在结果上不会出现这么戏剧性的一幕。其他数据一个道理，log和反log最大的好处就是能够让数据更加接近高斯分布，对于模型来说是一个比较好的数据。



**5.【snail】大神解答**



从两个方面进行分析

1）离散数据(count编码后的长尾)

对于离散数据处理，进行log是无用的。他只是改变了大小，但是没有改变概率分布。比如lgb：统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器, 然后遍历每个bin进行寻找最优分割点分裂，构建bin中根本没有对值的大小有操作；随机森林：统计entropy，cart，gain，也是只对熵进行运算。不过你如何数据变化，值始终是不变的。所以是不行的。



2）连续数值

对于连续数值进行log运算。可视化过后，你可以明显的看到，数据变平稳了。两端尾巴和峰值数据和相邻的数据距离变小了,方差也就变小了。这样可以更好的符合一定的分布（高斯，泊松…），对于模型泛化性就更好了。模型训练的时候。就不能够偏向一边进行分裂、比如lgb:leaf-wise，贪婪分裂叶子，造成只有一个分支。随机森林：也会有造成这样的结果。



总之，log是要分数据类型的。还有重要一点就是对数据特性也是有一定要求的。log后始终有一定破坏了数据的分布。一定考虑数据是否要进行log平滑。一般抖的长尾数据或者高斯分布…是要log平滑的。